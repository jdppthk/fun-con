{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swdb_2018_neuropixels.ephys_nwb_adapter import NWB_adapter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = '/data/dynamic-brain-workshop/visual_coding_neuropixels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_file = os.path.join(drive_path,'ephys_manifest.csv')\n",
    "expt_info_df = pd.read_csv(manifest_file)\n",
    "multi_probe_expt_info = expt_info_df[expt_info_df.experiment_type == 'multi_probe']\n",
    "multi_probe_example = 1 # index to row in multi_probe_expt_info\n",
    "multi_probe_filename  = multi_probe_expt_info.iloc[multi_probe_example]['nwb_filename']\n",
    "nwb_file = os.path.join(drive_path,multi_probe_filename)\n",
    "data_set = NWB_adapter(nwb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from downsampling_module import downsample_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_scenes = np.load('natural_scenes.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nat_scenes = np.array(downsample_images(nat_scenes, 25, 25))\n",
    "ds_nat_scenes = ds_nat_scenes*(1.0/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_nat_scenes[1].shape)\n",
    "plt.imshow(ds_nat_scenes[7], cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from downsampling_module import flatten_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_image_list = flatten_images(np.array(ds_nat_scenes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_table = data_set.get_stimulus_table('natural_scenes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_at_time(time, stim_table):\n",
    "    starts = stim_table.start.values\n",
    "    idx = np.searchsorted(starts, time)-1\n",
    "    return(stim_table.iloc[idx].values[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stim_time_array(stim_table, tns_start, tns_end, bin_len, flattened_image_list):\n",
    "    T = int(np.floor((tns_end - tns_start)/bin_len))\n",
    "    time_array = np.linspace(tns_start,tns_end,T)\n",
    "    stim_array = []\n",
    "    for idx, time_point in enumerate(time_array):\n",
    "        stim_index = get_frame_at_time(time_point, stim_table)\n",
    "        stim_array.append(flattened_image_list[int(stim_index)])\n",
    "    return(stim_array, time_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_len = 0.001\n",
    "num_stim_rows = 200\n",
    "tns_start = stim_table.iloc[0].values[0]\n",
    "tns_end = stim_table.iloc[num_stim_rows].values[1]\n",
    "print(tns_start)\n",
    "print(tns_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[stim_array, time_array] = get_stim_time_array(stim_table,tns_start,tns_end,bin_len,flattened_image_list)\n",
    "print(tns_start)\n",
    "print(tns_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(stim_array))\n",
    "\n",
    "print(np.shape(time_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(time_array.shape)\n",
    "# print(np.array(stim_array).shape)\n",
    "\n",
    "# time_array_short = time_array[0:len(time_array)/4]\n",
    "# stim_array_short = stim_array[:,:len(time_array)/4]\n",
    "# print(time_array_short.shape)\n",
    "# print(np.array(stim_array_short).shape)\n",
    "\n",
    "tns_start = time_array[0]\n",
    "tns_end = time_array[-1]\n",
    "print(tns_start)\n",
    "print(tns_end)\n",
    "\n",
    "print(time_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_spikes(data_set,bin_len,t_start,t_final,probes=None,regions=None):\n",
    "    if probes is None:\n",
    "        probes = data_set.probe_list\n",
    "    if regions is None:\n",
    "        regions = data_set.unit_df.structure.unique()\n",
    "    \n",
    "    #gather cells from desired regions and probes into cell_table\n",
    "    use_cells = False\n",
    "    for probe in probes:\n",
    "        for region in regions:\n",
    "            use_cells |= (data_set.unit_df.probe==probe) & (data_set.unit_df.structure==region)\n",
    "    cell_table = data_set.unit_df[use_cells]\n",
    "    \n",
    "    N = len(cell_table)     #number of cells\n",
    "    T = int(np.floor((t_final-t_start)/bin_len)) #number of time bins\n",
    "    binned_spikes = np.zeros((N,T)) # binned_spikes[i,j] is the number of spikes from neuron i in time bin j\n",
    "\n",
    "    #for each cell in the table, add each spike to the appropriate bin\n",
    "    i = 0\n",
    "    for z,cell in cell_table.iterrows(): \n",
    "        for spike_time in data_set.spike_times[cell['probe']][cell['unit_id']]:\n",
    "            t = int(np.floor((spike_time-t_start)/bin_len))\n",
    "            if (t >=0) & (t<T):\n",
    "                binned_spikes[i,t] += 1\n",
    "        i+=1    \n",
    "    return (binned_spikes, cell_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(binned_spikes, cell_table) = bin_spikes(data_set,bin_len,tns_start,tns_end,regions=['VISp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_spikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_binned_spikes = binned_spikes[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_binned_spikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.regularizers import Regularizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def GLM_network_fit(stimulus,spikes,d_stim, d_spk,bin_len,f='exp',priors=None,L1=None):\n",
    "    N = spikes.shape[0]\n",
    "    print(\"N\", N)\n",
    "    M = stimulus.shape[0]\n",
    "    print(\"M\", M)\n",
    "    F = np.empty((N,M,d_stim)) # stimulus filters\n",
    "    W = np.empty((N,N,d_spk))  # spike train filters\n",
    "    b = np.empty((N,)) # biases\n",
    "    fs = {'exp':K.exp}\n",
    "    Xdsn = construct_GLM_mat(stimulus,spikes, d_stim, d_spk)\n",
    "    for i in range(1):\n",
    "        y = spikes[i,max(d_stim,d_spk):]\n",
    "        # construct GLM model and return fit\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1,input_dim = Xdsn.shape[1],use_bias=True))\n",
    "        model.add(Lambda(lambda x: fs[f](x)*bin_len))\n",
    "        model.compile(loss = 'poisson',optimizer = keras.optimizers.adam(lr=5e-1))\n",
    "#        checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=False)\n",
    "        model.fit(x=Xdsn,y=y,epochs=5,verbose=1)\n",
    "        p = model.get_weights()[0]\n",
    "        F[i,:,:] = p[:M*d_stim].reshape((M,d_stim))\n",
    "        W[i,:,:] = p[M*d_stim:].reshape((N,d_spk))\n",
    "        b[i] = model.get_weights()[1]\n",
    "    return (F,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_regularizer=SparseGroupLasso(M*d_stim,d_spk,lgroup=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_GLM_mat(flat_stimulus, binned_spikes, d_stim, d_spk):\n",
    "    (N,T) = binned_spikes.shape # N is number of neurons, T is number of time bins\n",
    "    (M,T) = flat_stimulus.shape # M is the size of a stimulus\n",
    "    print(\"N,T\", (N,T))\n",
    "    print(\"M,T\", (M,T))\n",
    "    X_dsn = np.empty((T-d_stim,M*d_stim+N*d_spk))\n",
    "    d_max = max(d_stim,d_spk)\n",
    "    for t in range(T-d_max):\n",
    "        X_dsn[t,:M*d_stim] = np.fliplr(flat_stimulus[:,t+d_max-d_stim:t+d_max]).reshape((1,-1))  #stimulus inputs\n",
    "        X_dsn[t,M*d_stim:] = np.fliplr(binned_spikes[:,t+d_max-d_spk:t+d_max]).reshape((1,-1)) #spike inputs\n",
    "    return X_dsn    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import Regularizer\n",
    "from keras import backend as K\n",
    "\n",
    "class SparseGroupLasso(Regularizer):\n",
    "    \"\"\"Regularizer for group lasso regularization.\n",
    "    # Arguments\n",
    "       l1: Float; L1 regularization factor.\n",
    "       l2: Float; L2 group regularization factor.\n",
    "   \"\"\"\n",
    "\n",
    "    def __init__(self, size_stim, d_spike, lgroup = 1.):\n",
    "        self.lgroup = K.cast_to_floatx(lgroup)\n",
    "        self.d_spike = d_spike\n",
    "        self.size_stim = size_stim\n",
    "\n",
    "    def __call__(self, x): \n",
    "        xr = K.reshape(x[self.size_stim:], (-1, self.d_spike))\n",
    "        print(\"xrshape\", xr.shape)\n",
    "        return(self.lgroup * np.sqrt(K.int_shape(xr)[1])*K.sum(K.sqrt(K.sum(K.square(xr),axis=1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[F, W, b] = GLM_network_fit(np.array(stim_array).T,reduced_binned_spikes,20,20, bin_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('weights.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf['model_weights/dense_4/dense_4'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(W[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(norms, bins = 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = W[0][1]\n",
    "plt.plot(filter)\n",
    "print(\"norm\", np.linalg.norm(filter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtermat = W[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtermat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(filtermat[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.linalg.norm(filtermat, axis = 0), bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_GLM_mat(flat_stimulus, binned_spikes, i, d_stim, d_spk):\n",
    "#     (N,T) = binned_spikes.shape # N is number of neurons, T is number of time bins\n",
    "#     print(\"T\",T)\n",
    "#     (M,T) = flat_stimulus.shape # M is the size of a stimulus\n",
    "#     X_dsn = np.empty((T-d_stim+1,M*d_stim+N*d_spk))\n",
    "#     d_max = max(d_stim,d_spk)\n",
    "#     y = np.empty((T-d_max+1,))\n",
    "#     for t in range(T-d_max+1):\n",
    "#         y[t] = binned_spikes[i,t+d_max-1]\n",
    "#         X_dsn[t,:M*d_stim] = flat_stimulus[:,t+d_max-d_stim:t+d_max].reshape((1,-1))\n",
    "#         X_dsn[t,M*d_stim:] = binned_spikes[:,t+d_max-d_spk:t+d_max].reshape((1,-1))\n",
    "#     return (y, X_dsn)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras import backend as K\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input\n",
    "# from keras import Sequential\n",
    "# from keras.layers import Dense, Lambda\n",
    "# from keras.regularizers import Regularizer\n",
    "# def GLM_network_fit(stimulus,spikes,d_stim, d_spk,bin_len,f='exp',priors=None,L1=None):\n",
    "#     N = spikes.shape[0]\n",
    "#     print(\"N\", N)\n",
    "#     M = stimulus.shape[0]\n",
    "#     print(\"M\", M)\n",
    "#     F = np.empty((N,M,d_stim)) # stimulus filters\n",
    "#     W = np.empty((N,N,d_spk))  # spike train filters\n",
    "#     b = np.empty((N,)) # biases\n",
    "#     fs = {'exp':K.exp}\n",
    "#     for i in range(1):\n",
    "#         [y, Xdsn] = construct_GLM_mat(np.array(stim_array), binned_spikes, i, d_stim, d_spk)\n",
    "#         print(\"yshape\",y.shape)\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(1,input_dim = Xdsn.shape[1],use_bias=True, kernel_regularizer=SparseGroupLasso(M*d_stim,d_spk,lgroup=1e-10)))\n",
    "#         model.add(Lambda(lambda x: fs[f](x)*bin_len))\n",
    "#         model.compile(loss = 'poisson',optimizer = keras.optimizers.adam(lr=5e-1))\n",
    "#         model.fit(x=Xdsn,y=y,epochs=50, batch_size = 1000,  verbose=1)\n",
    "#         p = model.get_weights()[0]\n",
    "#         print(\"pshape\", p.shape)\n",
    "#         print(\"Mdstim\", M*d_stim)\n",
    "#         F[i,:,:] = p[:M*d_stim].reshape((M,d_stim))\n",
    "#         W[i,:,:] = p[M*d_stim:].reshape((N,d_spk))\n",
    "#         b[i] = model.get_weights()[1]\n",
    "#     return (F,W,b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
